{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "task 1\n"
      ],
      "metadata": {
        "id": "pAS4z2reZaQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Softmax Function:"
      ],
      "metadata": {
        "id": "cnPTskMDde6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZYd1xr6NI6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65cf70a3-c97b-4964-bb1b-3904277fc7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def logistic_function(x):\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "\n",
        "test_val = 0\n",
        "result = logistic_function(test_val)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Cases for Softmax Function:"
      ],
      "metadata": {
        "id": "3gLgCpQHZrvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def logistic_function(x):\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "\n",
        "def test_logistic_function():\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "\n",
        "\n",
        "\n",
        "test_logistic_function()"
      ],
      "metadata": {
        "id": "_4-txouzZvsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Categorical Log-Loss Function:"
      ],
      "metadata": {
        "id": "gEkHpu5jeDHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "    return loss\n",
        "\n",
        "print(log_loss(1, 0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksUEznlCd_il",
        "outputId": "689a6f82-8292-421c-dfe8-cca47878eff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case for log - loss function\n"
      ],
      "metadata": {
        "id": "D21A988yZv-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "    return loss\n",
        "\n",
        "def test_log_loss():\n",
        "    y_true = 1\n",
        "    y_pred = 0.2\n",
        "    expected_loss = -(1 * np.log(0.2)) - (0 * np.log(0.8))\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6)\n",
        "    print(log_loss(y_true, y_pred))\n",
        "\n",
        "test_log_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-OCWRCZyip",
        "outputId": "e5afdd83-931f-46e5-d56f-309afd72c407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6094379124341003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemenetation of Cost Function:"
      ],
      "metadata": {
        "id": "3g_HMHwGjQNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(y_true, y_pred):\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "    n = len(y_true)\n",
        "    loss_vec = log_loss(y_true, y_pred)\n",
        "    cost = np.sum(loss_vec) / n\n",
        "    return cost\n",
        "\n",
        "# Running one test case\n",
        "y_t = np.array([1, 0, 1])\n",
        "y_p = np.array([0.9, 0.1, 0.8])\n",
        "print(cost_function(y_t, y_p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwGoo4F4i4ro",
        "outputId": "28f0d041-3c40-4b9d-a1bc-3ce473c79890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14462152754328741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Cost Function:"
      ],
      "metadata": {
        "id": "S5RmFc8KjWG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(y_true, y_pred):\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "    n = len(y_true)\n",
        "    loss_vec = log_loss(y_true, y_pred)\n",
        "    cost = np.sum(loss_vec) / n\n",
        "    return cost\n",
        "\n",
        "def test_cost_function():\n",
        "    y_true = np.array([1, 0, 1])\n",
        "    y_pred = np.array([0.9, 0.1, 0.8])\n",
        "\n",
        "    expected_cost = (-(1 * np.log(0.9)) - (0 * np.log(0.1)) +\n",
        "                     -(0 * np.log(0.1)) - (1 * np.log(0.9)) +\n",
        "                     -(1 * np.log(0.8)) - (0 * np.log(0.2))) / 3\n",
        "\n",
        "    result = cost_function(y_true, y_pred)\n",
        "    assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "    print(result)\n",
        "\n",
        "test_cost_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVae1GdEjRw1",
        "outputId": "1e1a3c0c-ca6d-4f2e-e938-41370be77930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14462152754328741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Cost Function for Logistic/Sigmoid Regression"
      ],
      "metadata": {
        "id": "6aSzbB2qjeBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "    assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "\n",
        "    # Compute z using np.dot (z = Xw + b)\n",
        "    z = np.dot(X, w) + b\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute the cost using the cost function\n",
        "    cost = cost_function(y, y_pred)\n",
        "    return cost\n",
        "\n",
        "# Testing the Function:\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsvPnHX4jXm5",
        "outputId": "d23c9f52-62e2-4d6b-e9c3-e8e57f639021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Gradients for Sigmoid Regression:"
      ],
      "metadata": {
        "id": "m-lUMDtsjk9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "    assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute gradients\n",
        "    err = y_pred - y\n",
        "    grad_w = (1/n) * np.dot(X.T, err)\n",
        "    grad_b = (1/n) * np.sum(err)\n",
        "\n",
        "    return grad_w, grad_b\n",
        "\n",
        "# Running one test case\n",
        "X_test = np.array([[0.5, 0.2], [0.1, 0.8]])\n",
        "y_test = np.array([1, 0])\n",
        "w_test = np.array([0.1, -0.1])\n",
        "b_test = 0.0\n",
        "\n",
        "print(compute_gradient(X_test, y_test, w_test, b_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3yM0bbxjfC-",
        "outputId": "68f55df8-e05e-43fc-a72e-fb4591bc6b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.09899978,  0.1437528 ]), np.float64(-0.004996710057884712))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple assertion test for compute gradient function:\n",
        "\n"
      ],
      "metadata": {
        "id": "oEA7-vxgjqx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    err = y_pred - y\n",
        "    grad_w = (1/n) * np.dot(X.T, err)\n",
        "    grad_b = (1/n) * np.sum(err)\n",
        "\n",
        "    return grad_w, grad_b\n",
        "\n",
        "# Simple test case\n",
        "X = np.array([[10, 20], [-10, 10]])\n",
        "y = np.array([1, 0])\n",
        "w = np.array([0.5, 1.5])\n",
        "b = 1\n",
        "\n",
        "try:\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    print(\"Gradients computed successfully.\")\n",
        "    print(f\"grad_w: {grad_w}\")\n",
        "    print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "    print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9GtKvssjmTT",
        "outputId": "e419403e-2d9e-4078-ec4f-ee6b45900deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-4.99991649  4.99991649]\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent for Sigmoid Regression:"
      ],
      "metadata": {
        "id": "KeCj7cDyj4x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "    assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "\n",
        "        # Update weights and bias\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        # Compute cost\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        # Store cost and parameters\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        # Optionally print cost and parameters\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history\n",
        "\n",
        "# Test the gradient_descent function with sample data\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n",
        "y = np.array([1, 0])\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0.0\n",
        "alpha = 0.1\n",
        "n_iter = 1000\n",
        "\n",
        "# Perform gradient descent (reduced iterations for quick check)\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True, show_params=False)\n",
        "\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIAp8bq9jsJZ",
        "outputId": "0ecf7d44-6b4d-467a-bf0b-d91834b44ce6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING SOFTMAX FUNCTION\n",
            "======================================================================\n",
            "\n",
            "Test Case 1: Basic 2D Input (3 samples, 3 classes)\n",
            "----------------------------------------------------------------------\n",
            "Input logits:\n",
            "[[1. 2. 3.]\n",
            " [1. 1. 1.]\n",
            " [3. 2. 1.]]\n",
            "\n",
            "Softmax probabilities:\n",
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.33333333 0.33333333 0.33333333]\n",
            " [0.66524096 0.24472847 0.09003057]]\n",
            "\n",
            "Row sums (should all be 1.0): [1. 1. 1.]\n",
            "✓ PASSED\n",
            "\n",
            "======================================================================\n",
            "TESTING LOSS FUNCTION\n",
            "======================================================================\n",
            "\n",
            "True labels:  [0 1 0]\n",
            "Predictions:  [0.09 0.67 0.24]\n",
            "Loss:         0.400478\n",
            "Expected:     0.400478\n",
            "✓ PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1nHftZQ-NWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}