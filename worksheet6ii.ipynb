{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Softmax Function:"
      ],
      "metadata": {
        "id": "hJ-rDCPY-ATP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ybLkiozLkNHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5bbb18-517f-48fc-9909-9a874b2377ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [1. 2. 3.]\n",
            "Softmax: [0.09003057 0.24472847 0.66524096]\n",
            "Sum: 0.9999999999999999\n",
            "\n",
            "True label: [0 1 0]\n",
            "Prediction: [0.09003057 0.24472847 0.66524096]\n",
            "Loss: 1.4076059644443804\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    epsilon = 1e-10\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "z = np.array([[1.0, 2.0, 3.0]])\n",
        "probs = softmax(z)\n",
        "\n",
        "print(\"Input:\", z[0])\n",
        "print(\"Softmax:\", probs[0])\n",
        "print(\"Sum:\", np.sum(probs))\n",
        "\n",
        "y_true = np.array([0, 1, 0])\n",
        "y_pred = probs[0]\n",
        "loss = loss_softmax(y_true, y_pred)\n",
        "\n",
        "print(\"\\nTrue label:\", y_true)\n",
        "print(\"Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Cases for Softmax Function:"
      ],
      "metadata": {
        "id": "4I-nhqRE-XRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    epsilon = 1e-10\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "def test_softmax():\n",
        "    test_cases = [\n",
        "        (np.array([[0, 0, 0]]), \"All zeros\"),\n",
        "        (np.array([[1, 2, 3]]), \"Simple case\"),\n",
        "        (np.array([[1000, 1000, 1000]]), \"Large identical values\"),\n",
        "        (np.array([[-1000, -1000, -1000]]), \"Small identical values\"),\n",
        "        (np.array([[1, 0, -1]]), \"Mixed positive and negative\")\n",
        "    ]\n",
        "\n",
        "    for i, (z, description) in enumerate(test_cases):\n",
        "        print(f\"Test {i + 1}: {description}\")\n",
        "        result = softmax(z)\n",
        "\n",
        "        assert np.allclose(result.sum(axis=1), 1), f\"Failed: Probabilities do not sum to 1 in {description}\"\n",
        "        assert np.all(result >= 0), f\"Failed: Negative probabilities in {description}\"\n",
        "\n",
        "        print(f\"  Input: {z[0]}\")\n",
        "        print(f\"  Output: {result[0]}\")\n",
        "        print(f\"  Sum: {result.sum():.10f}\")\n",
        "        print(\"  Passed.\\n\")\n",
        "\n",
        "    print(\"All tests passed for softmax function.\")\n",
        "\n",
        "test_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfOfSDVv-DBG",
        "outputId": "9cd00fe9-ca4e-4770-84b0-32bd5e522dcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: All zeros\n",
            "  Input: [0 0 0]\n",
            "  Output: [0.33333333 0.33333333 0.33333333]\n",
            "  Sum: 1.0000000000\n",
            "  Passed.\n",
            "\n",
            "Test 2: Simple case\n",
            "  Input: [1 2 3]\n",
            "  Output: [0.09003057 0.24472847 0.66524096]\n",
            "  Sum: 1.0000000000\n",
            "  Passed.\n",
            "\n",
            "Test 3: Large identical values\n",
            "  Input: [1000 1000 1000]\n",
            "  Output: [0.33333333 0.33333333 0.33333333]\n",
            "  Sum: 1.0000000000\n",
            "  Passed.\n",
            "\n",
            "Test 4: Small identical values\n",
            "  Input: [-1000 -1000 -1000]\n",
            "  Output: [0.33333333 0.33333333 0.33333333]\n",
            "  Sum: 1.0000000000\n",
            "  Passed.\n",
            "\n",
            "Test 5: Mixed positive and negative\n",
            "  Input: [ 1  0 -1]\n",
            "  Output: [0.66524096 0.24472847 0.09003057]\n",
            "  Sum: 1.0000000000\n",
            "  Passed.\n",
            "\n",
            "All tests passed for softmax function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Categorical Log-Loss Function:"
      ],
      "metadata": {
        "id": "GPeOJT4E-hOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))\n",
        "\n",
        "y_true = np.array([0, 1, 0])\n",
        "y_pred = np.array([0.1, 0.8, 0.1])\n",
        "loss = loss_softmax(y_true, y_pred)\n",
        "expected = -np.log(0.8)\n",
        "\n",
        "print(\"True:\", y_true)\n",
        "print(\"Pred:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Expected:\", expected)\n",
        "print(\"Passed:\", np.isclose(loss, expected))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjo3t-zz-exm",
        "outputId": "815591e5-4ad3-4493-e7bc-68b4c607f53d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: [0 1 0]\n",
            "Pred: [0.1 0.8 0.1]\n",
            "Loss: 0.2231435511892097\n",
            "Expected: 0.2231435513142097\n",
            "Passed: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemenetation of Cost Function:"
      ],
      "metadata": {
        "id": "2A594qb7-mYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))\n",
        "\n",
        "def cost_softmax(X, y, W, b):\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / n\n",
        "\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "W = np.random.randn(2, 3)\n",
        "b = np.zeros(3)\n",
        "\n",
        "cost = cost_softmax(X, y, W, b)\n",
        "print(\"Cost:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJr1l8oR-9li",
        "outputId": "43e0e863-57b2-477d-ca41-2f8d710b62b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: 4.172775108476507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Cost Function:"
      ],
      "metadata": {
        "id": "4dHN9ZK9_F9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))\n",
        "\n",
        "def cost_softmax(X, y, W, b):\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / n\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "y = np.array([[1, 0], [0, 1], [1, 0]])\n",
        "W = np.array([[1, -1], [-1, 1]])\n",
        "b = np.array([0, 0])\n",
        "\n",
        "z = np.dot(X, W) + b\n",
        "y_pred = softmax(z)\n",
        "expected_cost = -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]\n",
        "calculated_cost = cost_softmax(X, y, W, b)\n",
        "\n",
        "print(\"Expected:\", expected_cost)\n",
        "print(\"Calculated:\", calculated_cost)\n",
        "print(\"Passed:\", np.isclose(calculated_cost, expected_cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dC9ycYl_DlK",
        "outputId": "05fcbf70-d813-4589-eef4-bc3cbd3771ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected: 1.460261343779191\n",
            "Calculated: 1.460261343779191\n",
            "Passed: True\n"
          ]
        }
      ]
    }
  ]
}